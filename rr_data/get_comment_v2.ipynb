{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a8eeb1-c82b-4c51-9d87-a9bb53ff80e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "示例帖子数据（含评论）:\n",
      "{'post_id': '1jiowpf', 'title': 'WTF is these memes', 'comments_count': 1}\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "# 初始化Reddit实例\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"84j0Vg5dY1skOG3wyGIbiA\",\n",
    "    client_secret=\"8fAD_w4V9X1BUD0_8Y-RcEjjVlacXQ\",\n",
    "    user_agent=\"RR Analytics/1.0 (by /u/Eric_Xu1025)\"\n",
    ")\n",
    "\n",
    "# 获取子论坛\n",
    "subreddit = reddit.subreddit(\"RajasthanRoyals\")\n",
    "\n",
    "# 收集帖子数据，包含post_id\n",
    "posts_data = []\n",
    "for post in subreddit.new(limit=10000):\n",
    "    posts_data.append({\n",
    "        \"post_id\": post.id,  # 新增post_id字段\n",
    "        \"title\": post.title,\n",
    "        \"author\": post.author.name if post.author else \"Unknown\",\n",
    "        \"score\": post.score,\n",
    "        \"comments_count\": post.num_comments,\n",
    "        \"url\": post.url,\n",
    "        \"text\": post.selftext,\n",
    "        \"comments\": []  # 初始化空评论列表\n",
    "    })\n",
    "\n",
    "# 创建字典以便通过post_id快速查找帖子\n",
    "posts_dict = {post[\"post_id\"]: post for post in posts_data}\n",
    "\n",
    "# 收集评论并关联到对应帖子\n",
    "for post in subreddit.new(limit=10000):  # 可根据需要调整范围（如遍历posts_dict中的帖子）\n",
    "    post_id = post.id\n",
    "    if post_id not in posts_dict:\n",
    "        continue  # 跳过未在初始搜索中的帖子\n",
    "    post.comments.replace_more(limit=0)  # 加载所有评论\n",
    "    for comment in post.comments.list():\n",
    "        comment_data = {\n",
    "            \"comment_author\": comment.author.name if comment.author else \"Anonymous\",\n",
    "            \"comment_body\": comment.body,\n",
    "            \"comment_score\": comment.score\n",
    "        }\n",
    "        posts_dict[post_id][\"comments\"].append(comment_data)\n",
    "\n",
    "# 生成时间戳\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 保存为CSV（可选）\n",
    "df_posts = pd.DataFrame(posts_data)\n",
    "df_posts.to_csv(f\"rr_posts_with_comments_{timestamp}.csv\", index=False)\n",
    "\n",
    "# 保存为JSON（结构化更清晰）\n",
    "with open(f\"rr_posts_with_comments_{timestamp}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(posts_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 打印示例数据（验证结构）\n",
    "print(\"示例帖子数据（含评论）:\")\n",
    "print({\n",
    "    \"post_id\": posts_data[0][\"post_id\"],\n",
    "    \"title\": posts_data[0][\"title\"],\n",
    "    \"comments_count\": len(posts_data[0][\"comments\"])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e24c05-d6cb-4358-8037-8abc03b11f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.7.1 of praw is outdated. Version 7.8.1 was released Friday October 25, 2024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase 1/3] Collecting post IDs...\n",
      "Collected 900 post IDs\n",
      "Total post IDs collected: 961\n",
      "\n",
      "[Phase 2/3] Collecting post details...\n",
      "Processed 961/961 posts | Comments: 11138\n",
      "Saved batch final to rr_data_20250325_063610_batch_final.json\n",
      "\n",
      "[Phase 3/3] Consolidating data files...\n",
      "Data consolidation logic can be implemented here\n",
      "\n",
      "Operation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"84j0Vg5dY1skOG3wyGIbiA\",\n",
    "    client_secret=\"8fAD_w4V9X1BUD0_8Y-RcEjjVlacXQ\",\n",
    "    user_agent=\"RR Analytics/1.0 (by /u/Eric_Xu1025)\"\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "SUBREDDIT_NAME = \"RajasthanRoyals\"\n",
    "TOTAL_POSTS = 10000\n",
    "SAVE_INTERVAL = 1000  # Save data every 1000 posts\n",
    "REQUEST_DELAY = 2     # Delay between API requests (seconds)\n",
    "\n",
    "def main():\n",
    "    # Phase 1: Collect post IDs first\n",
    "    print(\"[Phase 1/3] Collecting post IDs...\")\n",
    "    post_ids = []\n",
    "    \n",
    "    try:\n",
    "        for post in reddit.subreddit(SUBREDDIT_NAME).new(limit=TOTAL_POSTS):\n",
    "            post_ids.append(post.id)\n",
    "            time.sleep(0.5)  # Basic rate limiting\n",
    "            \n",
    "            # Progress tracking\n",
    "            if len(post_ids) % 100 == 0:\n",
    "                print(f\"Collected {len(post_ids)} post IDs\", end=\"\\r\")\n",
    "                \n",
    "        print(f\"\\nTotal post IDs collected: {len(post_ids)}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting post IDs: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Phase 2: Collect post details with comments\n",
    "    print(\"\\n[Phase 2/3] Collecting post details...\")\n",
    "    all_data = []\n",
    "    comment_counter = 0\n",
    "    \n",
    "    for idx, post_id in enumerate(post_ids, 1):\n",
    "        try:\n",
    "            # Get post object directly by ID\n",
    "            submission = reddit.submission(id=post_id)\n",
    "            \n",
    "            # Build post data structure\n",
    "            post_data = {\n",
    "                \"post_id\": submission.id,\n",
    "                \"title\": submission.title,\n",
    "                \"author\": submission.author.name if submission.author else \"Unknown\",\n",
    "                \"score\": submission.score,\n",
    "                \"comments_count\": submission.num_comments,\n",
    "                \"url\": submission.url,\n",
    "                \"text\": submission.selftext,\n",
    "                \"comments\": []\n",
    "            }\n",
    "            \n",
    "            # Process comments (with pagination handling)\n",
    "            submission.comments.replace_more(limit=None)\n",
    "            for comment in submission.comments.list():\n",
    "                post_data[\"comments\"].append({\n",
    "                    \"comment_id\": comment.id,\n",
    "                    \"author\": comment.author.name if comment.author else \"Anonymous\",\n",
    "                    \"body\": comment.body,\n",
    "                    \"score\": comment.score,\n",
    "                    \"created_utc\": comment.created_utc\n",
    "                })\n",
    "                comment_counter += 1\n",
    "            \n",
    "            all_data.append(post_data)\n",
    "            \n",
    "            # Progress tracking\n",
    "            print(f\"Processed {idx}/{len(post_ids)} posts | Comments: {comment_counter}\", end=\"\\r\")\n",
    "            \n",
    "            # Memory management: Save periodically\n",
    "            if idx % SAVE_INTERVAL == 0:\n",
    "                save_data(all_data, idx)\n",
    "                all_data = []  # Clear memory\n",
    "                \n",
    "            # Rate limiting\n",
    "            time.sleep(REQUEST_DELAY)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing post {post_id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Final save\n",
    "    if all_data:\n",
    "        save_data(all_data, \"final\")\n",
    "    \n",
    "    # Phase 3: Consolidate data\n",
    "    print(\"\\n[Phase 3/3] Consolidating data files...\")\n",
    "    consolidate_results()\n",
    "    \n",
    "def save_data(data, batch_id):\n",
    "    \"\"\"Save data to timestamped JSON file\"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"rr_data_{timestamp}_batch_{batch_id}.json\"\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"\\nSaved batch {batch_id} to {filename}\")\n",
    "\n",
    "def consolidate_results():\n",
    "    \"\"\"Combine all batch files into single dataset\"\"\"\n",
    "    # Implementation logic for combining files\n",
    "    # (Can be implemented based on specific needs)\n",
    "    print(\"Data consolidation logic can be implemented here\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(\"\\nOperation completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7ae754-5770-4472-9c1e-6e481f9e271d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
